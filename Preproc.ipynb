{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cb513e-1ef6-4cdd-8d84-9799a8be2d51",
   "metadata": {},
   "source": [
    "# Rock–Paper–Scissors Dataset Preprocessing\n",
    "\n",
    "This notebook scans the original dataset folders, validates images, and creates reproducible **train**, **validation**, and **test** splits.  \n",
    "It also generates a configuration JSON (`preprocess.json`) with resizing, normalization, and augmentation parameters used by all CNN models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042c4e2-48c1-4588-b9d9-f5a32875e221",
   "metadata": {},
   "source": [
    "## 1) Imports & Global Config\n",
    "Import the required libraries and define constants such as random seed and dataset path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124f9dd-66b5-4678-86ab-e42a8cd4a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import libraries and set global parameters (paths, random seed) ---\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import json, os\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671ba2d-9fd7-4e5b-8cc0-f732d2161129",
   "metadata": {},
   "source": [
    "## 2) Dataset Scanning & Validation\n",
    "Walk through the dataset directory, check for valid image files, collect metadata (path, label, width, height),  \n",
    "and handle any unreadable or corrupted files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8a1be-531f-47c3-8c0a-9ed2f51a0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Dir = Path(r\"Dataset/\")\n",
    "assert Dataset_Dir.exists(), f\"Dataset directory not found: {Dataset_Dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7c6b0-4cc6-45ff-aef7-0e1811213f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "corrupt = []\n",
    "EXTS = [\".png\", \".jpg\", \".jpeg\",\".bmp\",\".gif\"]\n",
    "\n",
    "class_dirs = [p for p in Dataset_Dir.iterdir() if p.is_dir()]\n",
    "class_names = sorted([p.name for p in class_dirs])\n",
    "print(\"Found class folder: \", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b715f7-48d6-4fde-ba27-b50437cd0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Iterate through all subfolders, check image validity, and record metadata ---\n",
    "\n",
    "for cdir in class_dirs:\n",
    "    label = cdir.name\n",
    "    for img_path in cdir.rglob(\"*\"):\n",
    "        if img_path.suffix.lower() not in EXTS:\n",
    "            continue\n",
    "        try:\n",
    "            with Image.open(img_path) as im:\n",
    "                im = im.convert(\"RGB\")\n",
    "                w, h = im.size\n",
    "            records.append({\"filepath\": str(img_path.resolve()), \"label\": label, \"width\": w, \"height\": h})\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            corrupt.append(str(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ecfa4-42c4-4d7d-ad13-ce783517d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "print(f\"\\nScanned {len(df)} images\\nCorrupt/unreadable: {len(corrupt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d050382-2c3a-4b79-813f-30ac37731146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCounts per class:\")\n",
    "print(df.groupby(\"label\").size().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb401f5-20a8-42dd-9a6a-b864dd7c1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnique (width, height) pairs found:\")\n",
    "print(df[[\"width\", \"height\"]].drop_duplicates().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e75085-0a70-4070-9014-9530e89f1153",
   "metadata": {},
   "source": [
    "## 3) Create Manifest File\n",
    "Combine all scanned images into a single dataframe and export it to `rps_outputs/manifest.csv` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d9aea-854d-4781-a7e6-c9b97944be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"rps_outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_dir / \"manifest.csv\", index=False)\n",
    "if corrupt:\n",
    "    with open(out_dir / \"corrupt_files.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(corrupt))\n",
    "print(\"\\nSaved:\", out_dir / \"manifest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ea97c-ee0f-4bbb-a383-1150d70433b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: df\n",
    "except NameError:\n",
    "    df = pd.read_csv(out_dir / \"manifest.csv\")\n",
    "\n",
    "X = df[\"filepath\"].values\n",
    "y = df[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7de33-42db-4312-a360-8a11068fb37c",
   "metadata": {},
   "source": [
    "## 4) Stratified Train/Val/Test Split\n",
    "Use `StratifiedShuffleSplit` to maintain class balance across **train**, **validation**, and **test** splits.  \n",
    "Ensure reproducibility via a fixed seed (42).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951f0db-2d5f-450b-b831-8d189f34f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Perform 2-level stratified split: (1) train+val vs test, (2) train vs val ---\n",
    "\n",
    "TEST_FRAC = 0.15\n",
    "VAL_FRAC = 0.15\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_FRAC, random_state=SEED)\n",
    "\n",
    "trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "X_trainval, y_trainval = X[trainval_idx], y[trainval_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7fd78-9399-4794-9c6d-8bcb2e898d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = VAL_FRAC / (1.0 - TEST_FRAC)\n",
    "\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=SEED)\n",
    "\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, y_train = X_trainval[train_idx], y_trainval[train_idx]\n",
    "X_val, y_val = X_trainval[val_idx], y_trainval[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca7f24-f5e3-4f3b-bf82-b1d3de8d52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"filepath\": X_train, \"label\": y_train})\n",
    "val_df = pd.DataFrame({\"filepath\": X_val, \"label\": y_val})\n",
    "test_df = pd.DataFrame({\"filepath\": X_test, \"label\": y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041ef67-83f0-44a6-8cb4-3c00c8df93f0",
   "metadata": {},
   "source": [
    "## 5) Export CSV Splits\n",
    "Save the generated splits as CSV files (`train.csv`, `val.csv`, `test.csv`) in `rps_outputs/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac35a4b-649e-4527-b269-b5bd13998f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(out_dir / \"train.csv\", index=False)\n",
    "val_df.to_csv(out_dir / \"val.csv\", index=False)\n",
    "test_df.to_csv(out_dir / \"test.csv\", index=False)\n",
    "\n",
    "print(\"Saved splits to:\", out_dir.resolve())\n",
    "print(\"Sizes -> Train:\", len(train_df), \" val:\", len(val_df), \" test:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5369ec4-0b74-4311-a223-b82f14769399",
   "metadata": {},
   "source": [
    "## 6) Sanity Checks\n",
    "Print per-class counts for all splits and verify there’s no overlap between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6448a-bdeb-45fa-8a37-4b1229209421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_counts(d, name):\n",
    "    c = d.groupby(\"label\").size().sort_index()\n",
    "    print(f\"\\n{name} per-class counts:\\n{c}\")\n",
    "\n",
    "show_counts(train_df, \"Train\")\n",
    "show_counts(val_df, \"Val\")\n",
    "show_counts(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13301be-72d0-48dd-8a1e-45a0ee42b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_train = set(train_df.filepath)\n",
    "set_val = set(val_df.filepath)\n",
    "set_test = set(test_df.filepath)\n",
    "\n",
    "assert set_train.isdisjoint(set_val) and set_train.isdisjoint(set_test) and set_val.isdisjoint(set_test), \"Overlap detected\"\n",
    "print(\"\\nNo overlap across splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3b771-a2bf-42c3-a6fd-fc1db66099ee",
   "metadata": {},
   "source": [
    "## 7) Preprocessing Configuration JSON\n",
    "Define and store preprocessing parameters (resize, normalize, augment) in `rps_outputs/preprocess.json`.  \n",
    "This ensures **consistency** between preprocessing and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39141c8b-2c7a-4cbf-8aa2-4aefbe3d21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save preprocessing settings (resize, normalization, augmentation) for later model use ---\n",
    "\n",
    "os.makedirs(\"rps_outputs\", exist_ok=True)\n",
    "\n",
    "PREPROC = {\n",
    "    \"seed\": 42,\n",
    "    \"img_size\": 128,\n",
    "    \"resize\": {\n",
    "        \"mode\": \"pad\",\n",
    "        \"width\": 128,\n",
    "        \"hright\": 128,\n",
    "        \"pad_color\": [0, 0, 0]\n",
    "    },\n",
    "    \"normalize\": {\n",
    "        \"type\": \"rescale\",\n",
    "        \"scale\": 1/255.0\n",
    "    },\n",
    "    \"augment\": {\n",
    "        \"flip_horizontal\": True,\n",
    "        \"rotation\": 0.08,\n",
    "        \"zoom\": 0.10,\n",
    "        \"contrast\": 0.10\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"rps_outputs/preprocess.json\", \"w\") as f:\n",
    "    json.dump(PREPROC, f, indent=2)\n",
    "\n",
    "print(\"Wrote rps_outputs/preprocess.json\")\n",
    "PREPROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe48afc-6d55-4f99-a74c-17c3530f76dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Outputs\n",
    "- `rps_outputs/manifest.csv` — Full dataset index  \n",
    "- `rps_outputs/train.csv`, `val.csv`, `test.csv` — Clean, stratified splits  \n",
    "- `rps_outputs/preprocess.json` — Shared configuration file  \n",
    "\n",
    "**Next step:** Run the model notebooks (`Model_A.ipynb` → `Model_D.ipynb`) using these CSVs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
